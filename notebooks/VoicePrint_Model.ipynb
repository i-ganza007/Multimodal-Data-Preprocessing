{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg49uKpRu23O6yQzXXzE87",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i-ganza007/Multimodal-Data-Preprocessing/blob/main/notebooks/VoicePrint_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e916056d",
        "outputId": "9a31024f-ed8b-4530-81cd-df94f87e02b6"
      },
      "source": [
        "!pip install audiomentations"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.11/dist-packages (0.42.0)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (2.0.2)\n",
            "Requirement already satisfied: numpy-minmax<1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.5.0)\n",
            "Requirement already satisfied: numpy-rms<1,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.6.0)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.12.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.11.0)\n",
            "Requirement already satisfied: python-stretch<1,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.3.1)\n",
            "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (1.16.0)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.12.0,>=0.8.0->audiomentations) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.svm import OneClassSVM\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import librosa\n",
        "from audiomentations import AddGaussianNoise"
      ],
      "metadata": {
        "id": "clGxkvjWbpyI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(audio_file, speaker=None, output_csv='/content/audio_features_augmented.csv'):\n",
        "    y, sr = librosa.load(audio_file)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    mfcc_mean = np.mean(mfcc, axis=1)\n",
        "    mfcc_std = np.std(mfcc, axis=1)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    mfcc_delta2_mean = np.mean(mfcc_delta2, axis=1)\n",
        "\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    rolloff_mean = np.mean(rolloff)\n",
        "    rolloff_std = np.std(rolloff)\n",
        "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    centroid_mean = np.mean(centroid)\n",
        "    centroid_std = np.std(centroid)\n",
        "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    bandwidth_mean = np.mean(bandwidth)\n",
        "    bandwidth_std = np.std(bandwidth)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    contrast_mean = np.mean(contrast)\n",
        "    contrast_std = np.std(contrast)\n",
        "    flatness = librosa.feature.spectral_flatness(y=y)\n",
        "    flatness_mean = np.mean(flatness)\n",
        "    flatness_std = np.std(flatness)\n",
        "\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "    rms_mean = np.mean(rms)\n",
        "    rms_std = np.std(rms)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
        "    zcr_mean = np.mean(zcr)\n",
        "    zcr_std = np.std(zcr)\n",
        "    f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "    f0_mean = np.mean(f0[~np.isnan(f0)]) if np.any(~np.isnan(f0)) else 0\n",
        "    f0_std = np.std(f0[~np.isnan(f0)]) if np.any(~np.isnan(f0)) else 0\n",
        "    f0_min = np.min(f0[~np.isnan(f0)]) if np.any(~np.isnan(f0)) else 0\n",
        "    f0_max = np.max(f0[~np.isnan(f0)]) if np.any(~np.isnan(f0)) else 0\n",
        "\n",
        "    order = 12\n",
        "    autocorr = np.correlate(y, y, mode='full')\n",
        "    autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+order+1]\n",
        "    r = autocorr[1:]\n",
        "    R = autocorr[:-1]\n",
        "    from scipy.linalg import solve_toeplitz\n",
        "    lpc_coeffs = solve_toeplitz((R, R), r)[:order]\n",
        "    lpc_coeffs = np.concatenate(([1], -lpc_coeffs))\n",
        "\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    chroma_mean = np.mean(chroma, axis=1)\n",
        "\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    onset_mean = np.mean(onset_env)\n",
        "    onset_std = np.std(onset_env)\n",
        "\n",
        "    features = {\n",
        "        'file': audio_file,\n",
        "        'speaker': speaker,\n",
        "        'duration': duration,\n",
        "        **{f'mfcc_{i}_mean': mfcc_mean[i] for i in range(13)},\n",
        "        **{f'mfcc_{i}_std': mfcc_std[i] for i in range(13)},\n",
        "        **{f'mfcc_delta_{i}_mean': mfcc_delta_mean[i] for i in range(13)},\n",
        "        **{f'mfcc_delta2_{i}_mean': mfcc_delta2_mean[i] for i in range(13)},\n",
        "        'rolloff_mean': rolloff_mean,\n",
        "        'rolloff_std': rolloff_std,\n",
        "        'centroid_mean': centroid_mean,\n",
        "        'centroid_std': centroid_std,\n",
        "        'bandwidth_mean': bandwidth_mean,\n",
        "        'bandwidth_std': bandwidth_std,\n",
        "        'contrast_mean': contrast_mean,\n",
        "        'contrast_std': contrast_std,\n",
        "        'flatness_mean': flatness_mean,\n",
        "        'flatness_std': flatness_std,\n",
        "        'rms_mean': rms_mean,\n",
        "        'rms_std': rms_std,\n",
        "        'zcr_mean': zcr_mean,\n",
        "        'zcr_std': zcr_std,\n",
        "        'f0_mean': f0_mean,\n",
        "        'f0_std': f0_std,\n",
        "        'f0_min': f0_min,\n",
        "        'f0_max': f0_max,\n",
        "        **{f'lpc_{i}': lpc_coeffs[i] for i in range(13)},\n",
        "        **{f'chroma_{i}_mean': chroma_mean[i] for i in range(12)},\n",
        "        'onset_mean': onset_mean,\n",
        "        'onset_std': onset_std\n",
        "    }\n",
        "    pd.DataFrame([features]).to_csv(output_csv, mode='a', index=False, header=not os.path.exists(output_csv))\n",
        "    return features"
      ],
      "metadata": {
        "id": "iWNo0QISbrCd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_audio(audio_file, speaker, output_csv='/content/audio_features_augmented.csv', n_augmentations=2):\n",
        "    y, sr = librosa.load(audio_file)\n",
        "    augment = AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1.0)\n",
        "    for i in range(n_augmentations):\n",
        "        y_aug = augment(y, sample_rate=sr)\n",
        "        augmented_file = f\"aug_{i}_{os.path.basename(audio_file)}\"\n",
        "        features = extract_features(audio_file, speaker=speaker, output_csv=output_csv)\n",
        "        if features is not None:\n",
        "            features['file'] = augmented_file\n",
        "            pd.DataFrame([features]).to_csv(output_csv, mode='a', index=False, header=not os.path.exists(output_csv))"
      ],
      "metadata": {
        "id": "8ZywUjp6b8oM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "try:\n",
        "    df = pd.read_csv('/content/audio_features.csv')\n",
        "    print(f\"Loaded {len(df)} samples\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: /content/audio_features.csv not found\")\n",
        "    exit()\n",
        "\n",
        "# Augment data using files in /content/audios with yes_approve_ and confirm_ prefixes\n",
        "speakers = df['speaker'].tolist()\n",
        "if os.path.exists('/content/audio_features_augmented.csv'):\n",
        "    os.remove('/content/audio_features_augmented.csv')\n",
        "for speaker in set(speakers):  # Unique speakers\n",
        "    for prefix in ['yes_approve_', 'confirm_']:\n",
        "        audio_file = f\"{prefix}{speaker}.wav\"\n",
        "        audio_path = os.path.join('/content/', audio_file)\n",
        "        if os.path.exists(audio_path):\n",
        "            augment_audio(audio_path, speaker, n_augmentations=2)\n",
        "        else:\n",
        "            print(f\"Warning: {audio_path} not found, skipping augmentation\")\n",
        "df = pd.read_csv('/content/audio_features_augmented.csv') if os.path.exists('/content/audio_features_augmented.csv') else df\n",
        "print(f\"Loaded {len(df)} samples after augmentation\")\n",
        "\n",
        "# Extract features\n",
        "feature_cols = [col for col in df.columns if col not in ['file', 'speaker', 'command', 'duration']]\n",
        "X = df[feature_cols].values\n",
        "file_names = df['file'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-svI8foacE5G",
        "outputId": "84a208ba-469f-4b98-9eff-a6f17554f1c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Loaded 8 samples\n",
            "Loaded 32 samples after augmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Assuming feature_cols and X are defined\n",
        "# And also y_true is your true labels array with 1 and -1 labels\n",
        "\n",
        "# Feature selection\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_selected = selector.fit_transform(X)\n",
        "selected_features = np.array(feature_cols)[selector.get_support()]\n",
        "print(f\"Selected {X_selected.shape[1]} features\")\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "# Prepare LOOCV\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "y_preds = []\n",
        "y_trues = []\n",
        "\n",
        "for train_idx, test_idx in loo.split(X_scaled):\n",
        "    X_fold_train, X_fold_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "    y_fold_train, y_fold_test = y_true[train_idx], y_true[test_idx]\n",
        "\n",
        "    # Train model on train fold\n",
        "    model = OneClassSVM(kernel='rbf', nu=0.1, gamma='auto')\n",
        "    model.fit(X_fold_train)\n",
        "\n",
        "    # Predict on test fold\n",
        "    y_pred = model.predict(X_fold_test)\n",
        "\n",
        "    y_preds.append(y_pred[0])\n",
        "    y_trues.append(y_fold_test[0])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_preds = np.array(y_preds)\n",
        "y_trues = np.array(y_trues)\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(y_trues, y_preds)\n",
        "prec = precision_score(y_trues, y_preds, pos_label=1)\n",
        "rec = recall_score(y_trues, y_preds, pos_label=1)\n",
        "f1 = f1_score(y_trues, y_preds, pos_label=1)\n",
        "\n",
        "print(f\"Accuracy: {acc:.3f}\")\n",
        "print(f\"Precision: {prec:.3f}\")\n",
        "print(f\"Recall: {rec:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")\n",
        "\n",
        "# Calculate inlier_counts\n",
        "inlier_counts = np.sum(y_preds == 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewKjw16HgmLt",
        "outputId": "d58280ad-1a0d-4988-a88b-a7b671288144"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 68 features\n",
            "Accuracy: 0.500\n",
            "Precision: 0.250\n",
            "Recall: 0.500\n",
            "F1 Score: 0.333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_dir = '/content/models'\n",
        "os.makedirs(model_output_dir, exist_ok=True)\n",
        "joblib.dump(model, f'{model_output_dir}/voiceprint_verification_model.pkl')\n",
        "joblib.dump(scaler, f'{model_output_dir}/voiceprint_scaler.pkl')\n",
        "joblib.dump(selected_features, f'{model_output_dir}/voiceprint_feature_columns.pkl')\n",
        "\n",
        "# Save metadata\n",
        "model_metadata = {\n",
        "    'model_type': 'OneClassSVM',\n",
        "    'feature_count': X_selected.shape[1],\n",
        "    'selected_features': selected_features.tolist(),\n",
        "    'sample_count': len(df),\n",
        "    'inlier_proportion': float(np.mean(inlier_counts)),\n",
        "    'datetime': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "with open(f'{model_output_dir}/voiceprint_model_metadata.json', 'w') as f:\n",
        "    json.dump(model_metadata, f, indent=2)\n",
        "\n",
        "print(f\"Model saved to {model_output_dir}/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOvrqkCxgwaq",
        "outputId": "818df779-51ed-456a-f84d-278dd6124d94"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURES_PATH = \"/content/models/voiceprint_feature_columns.pkl\"\n",
        "# SCALER_PATH = \"/content/models/voiceprint_scaler.pkl\"\n",
        "# MODEL_PATH = \"/content/models/voiceprint_verification_model.pkl\"\n",
        "\n",
        "# # Load model components once\n",
        "# feature_columns = joblib.load(FEATURES_PATH)\n",
        "\n",
        "\n",
        "# scaler = joblib.load(SCALER_PATH)\n",
        "# model = joblib.load(MODEL_PATH)\n",
        "\n",
        "# def get_expected_speaker_from_filename(filepath, accepted_list=None):\n",
        "#     filename = os.path.basename(filepath).lower()\n",
        "#     possible_names = accepted_list or ['eddy', 'ian', 'lievin', 'placide']\n",
        "#     for name in possible_names:\n",
        "#         if name in filename:\n",
        "#             return name\n",
        "#     return None\n",
        "\n",
        "# def verify_speaker(test_audio_path):\n",
        "#     # Step 1: Infer expected speaker\n",
        "#     expected_speaker = get_expected_speaker_from_filename(test_audio_path)\n",
        "#     if not expected_speaker:\n",
        "#         print(f\"Access Denied: Speaker name not found in file name '{test_audio_path}'\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"Testing '{test_audio_path}' for speaker '{expected_speaker}'...\")\n",
        "\n",
        "#     try:\n",
        "#         # Step 2: Extract features from the audio\n",
        "#         features_dict = extract_features(test_audio_path, speaker=None)\n",
        "#         feature_df = pd.DataFrame([features_dict])\n",
        "\n",
        "#         # Step 3: Match training feature columns\n",
        "#         for col in feature_columns:\n",
        "#             if col not in feature_df.columns:\n",
        "#                 feature_df[col] = 0  # fill missing\n",
        "#         feature_df = feature_df[feature_columns]\n",
        "\n",
        "#         # Step 4: Scale and predict\n",
        "#         scaled_input = scaler.transform(feature_df)\n",
        "#         predicted_speaker = model.predict(scaled_input)[0]\n",
        "\n",
        "#         # Step 5: Verification decision\n",
        "#         if predicted_speaker.lower() == expected_speaker.lower():\n",
        "#             print(f\"Access Granted: Speaker verified as '{predicted_speaker}'\")\n",
        "#         else:\n",
        "#             print(f\"Access Denied: Expected '{expected_speaker}', but got '{predicted_speaker}'\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error verifying speaker: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "h9l2yk4Gh4Q2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5f32dec",
        "outputId": "c10438b1-5c55-42b3-9844-822cd7f7d882"
      },
      "source": [
        "# Extract features dictionary for a new audio file\n",
        "features_dict = extract_features('/content/isaac (online-audio-converter.com).wav', speaker='unknown')\n",
        "\n",
        "if features_dict is None:\n",
        "    print(\"Feature extraction failed.\")\n",
        "else:\n",
        "    # Convert to DataFrame (1-row)\n",
        "    import pandas as pd\n",
        "    test_df = pd.DataFrame([features_dict])\n",
        "\n",
        "feature_cols = joblib.load('/content/models/voiceprint_feature_columns.pkl')\n",
        "scaler = joblib.load('/content/models/voiceprint_scaler.pkl')\n",
        "\n",
        "# Select only the features the model was trained on\n",
        "X_new = test_df[feature_cols].values\n",
        "\n",
        "# Scale features\n",
        "X_new_scaled = scaler.transform(X_new)\n",
        "\n",
        "model = joblib.load('/content/models/voiceprint_verification_model.pkl')\n",
        "\n",
        "# Predict (1 = accepted/inlier, -1 = rejected/outlier)\n",
        "prediction = model.predict(X_new_scaled)[0]\n",
        "\n",
        "if prediction == 1:\n",
        "    print(\"Audio accepted: Speaker is authorized.\")\n",
        "else:\n",
        "    print(\"Audio rejected: Speaker is NOT authorized.\")\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio rejected: Speaker is NOT authorized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hck0zSCxofAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6x2_NLfSolcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}